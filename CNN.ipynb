{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"213070015_213079003_4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**4. Non-linear dimension reduction:**\n","a. Visualize the data from the file DataKPCA.csv.\n","\n","b. Train KPCA. \n","\n","c. Plot the variance explained versus KPCA dimensions for up to 10 dimensions. "],"metadata":{"id":"6iu0HkULWSe1"}},{"cell_type":"markdown","source":["##**Import the libraries**"],"metadata":{"id":"9Qsg7ZQJR_88"}},{"cell_type":"code","source":["\n","from sklearn.decomposition import KernelPCA \n","from keras.datasets import mnist\n","from sklearn.datasets import load_iris\n","from numpy import reshape\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import plotly.express as px"],"metadata":{"id":"g_Nu_8CzaEW1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Data Visualization**"],"metadata":{"id":"J7L97A19SdRN"}},{"cell_type":"code","source":["#Interpretation for correlation matrix\n","df = pd.read_csv(\"/content/drive/MyDrive/DataKPCA.csv\")\n","print(df.corr())\n","plt.figure(figsize=(10,10))\n","dataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n","plt.show()"],"metadata":{"id":"N-X1nrBJGdFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bOjpEAgxqngU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Observations**\n","*   x1 is highly correlated with x3 and x4\n","*   x2 is uncorrelated with other features\n","*   x3 is highly correlated with x1,x4,x5 and x6\n","*   x4 is highly correlated with x1 and x3\n","*   x5 is highly correlated with x2 and x3 \n","*   x6 is highly correlated with x3 and x5\n","*   x7 is uncorrelated with other features\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"FP8UTnXtS2rt"}},{"cell_type":"code","source":["#Interpretation from scatter plot\n","df = pd.read_csv(\"/content/drive/MyDrive/DataKPCA.csv\")\n","x=np.arange(0,df.shape[0],1)\n","plt.figure(figsize=(5,5))\n","for i in range(df.shape[1]):\n","  p=str(i+1)\n","  plt.scatter(x,df.iloc[:,i],label='x'+p)\n","  plt.legend()"],"metadata":{"id":"G7YccC5aaEaW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can tell from the scatter plot that the features have less variance because they have less variation in the y axis."],"metadata":{"id":"WUrxi1ABW_IV"}},{"cell_type":"markdown","source":["##**KPCA**"],"metadata":{"id":"p0JMLy6UW--F"}},{"cell_type":"code","source":["#Defining kpca function for geting PC's\n","def kpca(data,no_components,gamma_value): \n","   pca = KernelPCA(kernel=\"rbf\", n_components=no_components, gamma=gamma_value)\n","   z = pca.fit_transform(x)\n","   df = pd.DataFrame(z)\n","   explained_variance = list(np.var(df, axis=0))\n","   return z   "],"metadata":{"id":"JGgZShXlae28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x=pd.read_csv(\"/content/drive/MyDrive/DataKPCA.csv\") #reading Data\n","################################\n","\"\"\"Using the PCA function, we can obtain the first principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_1 = kpca(x,1,0.01)\n","principal_df1 = pd.DataFrame(kpca_df_1, columns = ['PC1'])\n","print(principal_df1)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first two principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_2= kpca(x,2,0.01)\n","principal_df2 = pd.DataFrame(kpca_df_2, columns = ['PC1','PC2'])\n","print(principal_df2)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first three principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_3 = kpca(x,3,0.01)\n","principal_df3 = pd.DataFrame(kpca_df_3, columns = ['PC1','PC2','PC3'])\n","print(principal_df2)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first four principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_4 = kpca(x,4,0.01)\n","principal_df4 = pd.DataFrame(kpca_df_4, columns = ['PC1','PC2','PC3','PC4'])\n","print(principal_df4)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first five principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_5= kpca(x,5,0.01)\n","principal_df5 = pd.DataFrame(kpca_df_5, columns = ['PC1','PC2','PC3','PC4','PC5'])\n","print(principal_df5)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first six principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_6 = kpca(x,6,0.01)\n","principal_df6 = pd.DataFrame(kpca_df_6, columns = ['PC1','PC2','PC3','PC4','PC5','PC6'])\n","print(principal_df6)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first seven principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_7 = kpca(x,7,0.01)\n","principal_df7 = pd.DataFrame(kpca_df_7, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7'])\n","print(principal_df7)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first eight principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_8 = kpca(x,8,0.01)\n","principal_df8 = pd.DataFrame(kpca_df_8, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8'])\n","print(principal_df8)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first nine principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_9 =kpca(x,9,0.01)\n","principal_df1 = pd.DataFrame(kpca_df_9, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9'])\n","print(principal_df1)\n","################################\n","\"\"\"Using the PCA function, we can obtain the first ten principle components and their mse by subtracting the reconstructed matrix.\"\"\"\n","kpca_df_10= kpca(x,10,0.01)\n","principal_df10 = pd.DataFrame(kpca_df_10, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10'])\n","print(principal_df10)"],"metadata":{"id":"6ipJOrQ6aezj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Call PCA function\n","pca = KernelPCA(n_components=10, kernel=\"rbf\")\n","# Transform features\n","pca.fit(df)\n","# Determine explained variance using scikit learn\n","var_pca = pca.eigenvalues_ / sum(pca.eigenvalues_)\n","print(sum(var_pca))\n","# The cumulative sum of eigenvalues will be used to generate a step plot.\n","# In order to visualise the variance explained by each principal component\n","cumsum_eig = np.cumsum(var_pca)\n","#\n","# Create the visualization plot\n","#\n","plt.bar(range(0,len(var_pca)), var_pca, alpha=0.5, align='center', label='Individual explained variance')\n","plt.step(range(0,len(cumsum_eig)), cumsum_eig, where='mid',label='Cumulative explained variance')\n","plt.ylabel('Explained variance ratio')\n","plt.xlabel('Principal component index')\n","plt.legend(loc='best')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"qrl5zlAJ1W7L"},"execution_count":null,"outputs":[]}]}